{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c893f",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q --force-reinstall airavata-python-sdk[notebook]\n",
    "import airavata_jupyter_magic\n",
    "\n",
    "%authenticate\n",
    "%request_runtime hpc_cpu --file=cybershuttle.yml --walltime=60 --use=NeuroData25VC1:cloud,expanse:shared,anvil:shared\n",
    "%switch_runtime hpc_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd7f11a-1301-464a-87e6-62768c530596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a567c016-c18a-4bd3-b871-c9880a64a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_id = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "text2text_pipeline = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f540d84",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18492ef-42c1-4348-853a-efcb934e53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    response = text2text_pipeline(\"What are competitors to Apache Kafka?\")\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5108d007-1e35-4c10-a196-0723bc1af704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure CPU usage before and after running the model\n",
    "cpu_before = psutil.cpu_percent(interval=1)\n",
    "start_time = time.time()\n",
    "response = run_model()\n",
    "end_time = time.time()\n",
    "cpu_after = psutil.cpu_percent(interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92daef88-e669-4414-abdf-6af32061e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage = memory_usage(run_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3c47ce-8d31-4b4a-b6b2-797f9d0a90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 37.58616542816162 seconds\n",
      "Peak Memory Usage: 12156.91015625 MiB\n",
      "CPU Before: 19.9%\n",
      "CPU After: 22.2%\n",
      "Model Output: [{'generated_text': 'Apache Kafka is a popular open source message broker that is used for streaming data processing and streaming applications. Some of its competitors include Apache Spark, Apache Storm, Apache Flink, Apache Flume, and Apache Flink Streams.'}]\n"
     ]
    }
   ],
   "source": [
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time: {execution_time} seconds\")\n",
    "print(f\"Peak Memory Usage: {max(mem_usage)} MiB\")\n",
    "print(f\"CPU Before: {cpu_before}%\")\n",
    "print(f\"CPU After: {cpu_after}%\")\n",
    "print(f\"Model Output: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
